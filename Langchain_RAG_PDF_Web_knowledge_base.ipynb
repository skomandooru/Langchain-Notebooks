{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Doing Retrieval Augmented Generation with Langchain using knowledge base populated from a PDF or Web to answer questions. This solution uses an in memory vector store, Claude Sonnet 3.5 LLM for generation and Amazon Titan Embed LLM for indexing/retrieval.\n",
        "\n",
        "[Langchain documentation](https://python.langchain.com/docs/tutorials/rag/)\n"
      ],
      "metadata": {
        "id": "-a0JKWgoBdwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installed all the necessary libraries for to run with Langchain"
      ],
      "metadata": {
        "id": "L3ZoYUxSp9FY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9lUqNwBDkvja"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph langchain-aws langchain-core langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure your AWS credentials are configured\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\")"
      ],
      "metadata": {
        "id": "mWlTtsQGrO0V"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select your embeddings model \"AWS\""
      ],
      "metadata": {
        "id": "_x_6aPhdqbpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_aws import BedrockEmbeddings\n",
        "\n",
        "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")"
      ],
      "metadata": {
        "id": "L34TkIRSrSuK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can select your vector store. It can be \"in-memory\" or \"FAISS\""
      ],
      "metadata": {
        "id": "epm4KHyhrier"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "J6HzfRdFr9lA"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a pdf that I uploaded which is called edfors-et-al-gene. The following code will load the file, split it into several chunks, and store it into the vector database. It also does the retrieval step in Retrieval Augmented Generation."
      ],
      "metadata": {
        "id": "u7fQhl-vsAvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retrieval step is pulling stored info from the database. The question that the user asks is tranformed into vectors, these vectors are matched to the ones in the database, and then it asks the llm to answer the question in text."
      ],
      "metadata": {
        "id": "yHwsfp8IFxhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load and chunk contents of the PDF\n",
        "\n",
        "file_path = \"/content/rag-langchain/edfors-et-al-gene.pdf\"  # Replace with your PDF path\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\"\"\"\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
        "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jack1SDRseoi",
        "outputId": "29eb32f9-c74c-4f85-e974-3ae204737f5d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM will generate an answer based on the user's prompt."
      ],
      "metadata": {
        "id": "2LV6clk-EJxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "response = graph.invoke({\"question\": \"What are the RNA and protein levels were studied in samples from nine human cell lines (Table EV1) and 11 human tissues representing?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUafZq9tsmlX",
        "outputId": "a8143401-b133-45d2-df9b-6d6a43063638"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context provided, the RNA and protein levels were studied in samples from nine human cell lines and 11 human tissues. The study aimed to compare absolute protein copy numbers with corresponding mRNA levels across these samples. However, the specific details about what the 11 tissues represent are not given in the provided context.\n"
          ]
        }
      ]
    }
  ]
}