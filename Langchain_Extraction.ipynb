{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Used Langchain, Claude Sonnet 3.5 LLM, Amazon Bedrock, and tool-calling features of chat models to extract structured information from unstructured text while using the langchain documentation.\n",
        "\n",
        "[Langchain Extraction](https://python.langchain.com/docs/tutorials/extraction/#the-schema)"
      ],
      "metadata": {
        "id": "76Sx1CiFWNUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Case is an Automated Resume Screening for Recruiters. Recruiters can use a chat model with tool-calling to extract structured data (like name, skills, work experience) from unstructured resumes. This helps them quickly process and analyze candidate information for Applicant Tracking Systems (ATS)."
      ],
      "metadata": {
        "id": "eohjc8gffGyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install the necessary packages and put in the AWS credentials!"
      ],
      "metadata": {
        "id": "d9rj6WOKZatS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGp0S0aRWHfm"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade langchain-core langchain-aws"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to describe what information we want to extract from the text.\n",
        "We'll use Pydantic to define an example schema to extract personal information."
      ],
      "metadata": {
        "id": "GuxdHpZVZ97R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        default=None, description=\"The color of the person's hair if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(\n",
        "        default=None, description=\"Height measured in meters\"\n",
        "    )"
      ],
      "metadata": {
        "id": "_OJEUFC-aFxn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two best practices when defining schema:\n",
        "\n",
        "\n",
        "1. Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\n",
        "2. Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer."
      ],
      "metadata": {
        "id": "i56Vc4fXaJ8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an information extractor using the schema we defined above."
      ],
      "metadata": {
        "id": "eAGxqoNnabbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
        "#    about the document from which the text was extracted.)\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. \"\n",
        "            \"Only extract relevant information from the text. \"\n",
        "            \"If you do not know the value of an attribute asked to extract, \"\n",
        "            \"return null for the attribute's value.\",\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with\n",
        "        # reference examples.\n",
        "        # MessagesPlaceholder('examples'),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "hLDByvm3agMk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to use a model that supports function/tool calling."
      ],
      "metadata": {
        "id": "LvrXu2ZxakNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure your AWS credentials are configured\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\")"
      ],
      "metadata": {
        "id": "iaUqVZsFao5E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "d7qQu7TYavlH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPlmIQSiayNX",
        "outputId": "061dea18-0c09-4724-bb59-a72f6e340894"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableBinding(bound=ChatBedrockConverse(client=<botocore.client.BedrockRuntime object at 0x7f03c9d20f10>, model_id='anthropic.claude-3-5-sonnet-20240620-v1:0', aws_access_key_id=SecretStr('**********'), aws_secret_access_key=SecretStr('**********'), aws_session_token=SecretStr('**********'), provider='anthropic', supports_tool_choice_values=('auto', 'any', 'tool')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Person', 'description': 'Information about a person.', 'parameters': {'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'The name of the person'}, 'hair_color': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"The color of the person's hair if known\"}, 'height_in_meters': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Height measured in meters'}}, 'type': 'object'}}}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling'}, 'schema': {'type': 'function', 'function': {'name': 'Person', 'description': 'Information about a person.', 'parameters': {'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'The name of the person'}, 'hair_color': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"The color of the person's hair if known\"}, 'height_in_meters': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Height measured in meters'}}, 'type': 'object'}}}}, 'tool_choice': {'tool': {'name': 'Person'}}}, config={}, config_factories=[])\n",
              "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.Person'>])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out:"
      ],
      "metadata": {
        "id": "oAwymGnaa7gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaI6UPeIa9XA",
        "outputId": "bc40d38e-6283-4e0d-d3c1-13c8be65e6ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most cases, you should be extracting a list of entities rather than a single entity.\n",
        "\n",
        "This can be easily achieved using pydantic by nesting models inside one another."
      ],
      "metadata": {
        "id": "AgiiIrw6bJpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
        "    # and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(\n",
        "        default=None, description=\"The color of the person's hair if known\"\n",
        "    )\n",
        "    height_in_meters: Optional[str] = Field(\n",
        "        default=None, description=\"Height measured in meters\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people: List[Person]"
      ],
      "metadata": {
        "id": "JtGFiyx2bLbc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens!"
      ],
      "metadata": {
        "id": "FAIz24qrbOSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWokzemabU8K",
        "outputId": "a9715449-e75e-44ae-a0ee-e789f68e775f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The behavior of LLM applications can be steered using few-shot prompting. For chat models, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.\n",
        "\n",
        "For example, we can convey the meaning of a symbol with alternating user and assistant messages:"
      ],
      "metadata": {
        "id": "u5rXFuNHcZwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"2 🦜 2\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"4\"},\n",
        "    {\"role\": \"user\", \"content\": \"2 🦜 3\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"5\"},\n",
        "    {\"role\": \"user\", \"content\": \"3 🦜 4\"},\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSvhs6D6cbvf",
        "outputId": "b151e565-5028-428f-a8ec-b7a450d7e849"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason why it's 7 is because the chili like character in between the pairs seems to be acting like a + sign. If 2 + 2 = 4, and 2 + 3 = 5, then 3 + 4 must be 7."
      ],
      "metadata": {
        "id": "mGchjw-1clA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structured output often uses tool calling under-the-hood. This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls. What should a sequence of messages look like in this case?\n",
        "\n",
        "Different chat model providers impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:\n",
        "\n",
        "\n",
        "* User message\n",
        "* AI message with tool call\n",
        "* Tool message with result\n",
        "\n",
        "Others require a final AI message containing some sort of response."
      ],
      "metadata": {
        "id": "H03-bgRrgYZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain includes a utility function tool_example_to_messages that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.\n",
        "\n",
        "Let's try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider's required format."
      ],
      "metadata": {
        "id": "QHC-yTptgwMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import tool_example_to_messages\n",
        "\n",
        "examples = [\n",
        "    (\n",
        "        \"The ocean is vast and blue. It's more than 20,000 feet deep.\",\n",
        "        Data(people=[]),\n",
        "    ),\n",
        "    (\n",
        "        \"Fiona traveled far from France to Spain.\",\n",
        "        Data(people=[Person(name=\"Fiona\", height_in_meters=None, hair_color=None)]),\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "\n",
        "for txt, tool_call in examples:\n",
        "    if tool_call.people:\n",
        "        # This final message is optional for some providers\n",
        "        ai_response = \"Detected people.\"\n",
        "    else:\n",
        "        ai_response = \"Detected no people.\"\n",
        "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))"
      ],
      "metadata": {
        "id": "sn9xW8TBg1RZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the result, we see these two example pairs generated eight messages:"
      ],
      "metadata": {
        "id": "cahHqMaxg858"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for message in messages:\n",
        "    message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgYCrLHqg562",
        "outputId": "bbf9e5dd-a056-4933-c5d7-15bbd27a2238"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "The ocean is vast and blue. It's more than 20,000 feet deep.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Data (fab79d47-b259-476c-bf65-35c12ad1f8a2)\n",
            " Call ID: fab79d47-b259-476c-bf65-35c12ad1f8a2\n",
            "  Args:\n",
            "    people: []\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "You have correctly called this tool.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Detected no people.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Fiona traveled far from France to Spain.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Data (25434004-2a14-4e41-aecc-e11ffbbb7561)\n",
            " Call ID: 25434004-2a14-4e41-aecc-e11ffbbb7561\n",
            "  Args:\n",
            "    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "You have correctly called this tool.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Detected people.\n"
          ]
        }
      ]
    }
  ]
}